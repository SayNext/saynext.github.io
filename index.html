<!DOCTYPE html>
<html>

<head>
  <title>
    SAYNEXT: A Benchmark and Cognitively Inspired Framework for Next-Utterance
    Prediction with Multimodal LLMs
  </title>
  <!-- consider to add our icon here -->
  <!-- <link rel="icon" href="" type="image/icon type"> -->

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
  <script
    src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="web/css/bulma.min.css" />
  <link rel="stylesheet" href="web/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="web/css/bulma-slider.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="web/css/fontawesome.all.min.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./web/js/bulma-carousel.min.js"></script>
  <script src="./web/js/bulma-slider.min.js"></script>

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous" />
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
    crossorigin="anonymous"></script>

  <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet" />
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
  <script defer src="web/js/fontawesome.all.min.js"></script>

  <!-- below we load some js scripts -->
  <script src="web/js/leaderboard.js" type="module"></script>

  <link rel="stylesheet" href="web/css/index.css" />

  <!-- MathJax script -->
  <!-- <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/javascript">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
    });
  </script> -->

  <script>
    document.addEventListener("DOMContentLoaded", function () {
      var toggles = document.querySelectorAll(".toggle-section");
      toggles.forEach(function (toggle) {
        toggle.addEventListener("click", function () {
          var content = document.getElementById(
            toggle.getAttribute("aria-controls")
          );
          var toggleIcon = toggle.children[1].children[0];
          content.classList.toggle("is-active");
          if (content.classList.contains("is-active")) {
            toggleIcon.style.transition = "transform 0.3s ease";
            toggleIcon.style.transform = "rotate(180deg)";
          } else {
            toggleIcon.style.transition = "transform 0.3s ease";
            toggleIcon.style.transform = "rotate(0deg)";
          }
        });
      });
    });
  </script>

  <style>
    .collapse-content {
      display: none;
      margin-top: 10px;
    }

    .collapse-content.is-active {
      display: block;
    }

    /* .toggle-section .icon.is-small {
          transition: transform 0.3s ease;
        } */
    /* .toggle-section .fa-angle-up {
          transform: rotate(180deg);
        } */
  </style>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
              <!-- <img src="website/img/mint-leaf-logo.png" alt="logo" width="40" height="40" /> -->
              SAYNEXT: A Benchmark and Cognitively Inspired Framework for
              Next-Utterance Prediction with Multimodal LLMs
            </h1>
            <!-- <br/> -->
            <p style="color: darkred; font-weight: bold; font-size: larger;">Guess what I’m gonna say!</p>

            <!-- <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <a href="???">???</a><sup>1</sup>,</span>
                        </div>

                        <div class="'is-size-4 publication-authors">
                        <span class="author-block" style="color: darkred; font-weight: bold;">Conference</span>
                        </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                                <a class="btn btn-outline-dark"
                                    role="button">
                                &nbsp;
                                    <i class="fas fa-file-pdf"></i>
                                    <span>&nbsp;&nbsp;Paper (Coming Soon)</span>
                                </a> &nbsp;&nbsp;
                                </span> -->

                <!-- Arxiv Link. -->
                <!-- <span class="link-block">
                                <a href="https://arxiv.org/abs/" class="btn btn-outline-dark"
                                    role="button">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv</span>
                                </a> &nbsp;&nbsp;
                            </span> -->

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/SayNext/SayNext" class="btn btn-outline-dark" role="button">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  &nbsp;&nbsp;
                </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/SayNext/SayNext-PC2K" class="btn btn-outline-dark"
                    role="button" style="display: inline-flex; align-items: center">
                    <span class="icon" style="display: inline-flex; align-items: center">
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face"
                        style="width: 20px; height: 18px; margin-right: 5px" />
                    </span>
                    <span>Dataset</span>
                  </a>
                  &nbsp;&nbsp;
                </span>
              </div>
            </div>

            <!-- <br/> -->
            <br />

            <div class="content has-text-justified">
              <figure>
                <img src="web/img/main.png" style="padding: 10px 0px; width: 90%;"
                  alt="SayNext-Bench performance overview." />
                <figcaption style="text-align: left; display: block; width: 90%; margin: auto;">
                  <b>Figure 1:</b>
                  <b>The Illustration of Next-Utterance Prediction.</b> Given a
                  video and the interviewer’s question turn, the SayNext-Chat
                  predicts the subsequent response using a dual-route framework.
                  In evaluation, key factors predicted by our model (<span style="color: #009900">green</span>) align
                  with
                  those in the ground truth (<span style="color: #0066cc">blue</span>), whereas other MLLMs often yield
                  irrelevant factors,
                  incorrect ones, or entirely fail to predict (<span style="color: #cc0000">red</span>).
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="abstract">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-2">Abstract</h2>
          <div class="content has-text-justified content-larger-text">
            <p><b>Problem: </b>We explore the use of large language models (LLMs) for next-utterance prediction in human
              dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations
              with users, we show that even leading models surprisingly struggle to predict a human speaker’s next
              utterance. Instead, humans can readily anticipate forthcoming utterances based on multi-modal cues—such as
              gestures, gaze, and emotional tone—from the context.
            </p>
            <p><b>Method: </b>
              To systematically examine whether LLMs can reproduce this ability, we propose
              <b>SayNext-Bench</b>, a benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating
              context-conditioned responses from multimodal cues spanning a variety of real-world scenarios. To support
              this benchmark, we build <b>SayNext-PC</b>, a novel large-scale dataset containing dialogues with rich
              multimodal cues. Building on this, we further develop a dual-route prediction MLLM, <b>SayNext-Chat</b>,
              that incorporates cognitive-inspired design to emulate the predictive processing in conversation.
              Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical
              overlap, semantic similarity, and emotion consistency. Our results verify the feasibility of
              next-utterance prediction with LLMs from multimodal cues, and emphasize the indispensable role of
              non-verbal cues as the foundation of natural human interaction.
            </p>
            <p><b>Contribution: </b>
              We present the <b>SAYNEXT</b> and position it as an initial step toward bridging the gap
              between fluent dialogue generation and genuine cognitive understanding in human–AI interaction. We believe
              this exploration not only opens a new direction toward more human-like, context-sensitive AI interaction
              but also offers a pathway to uncovering cognitive concepts from dialogue data for human-centered AI.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <h2 class="title is-2" style="text-align: center">Interactive Demo</h2>
      <div class="hero-body">
        <div style="width: 90%; margin: auto;">
          <p><b>Tip: </b>For the best experience, please turn on the audio while watching the demo.</p>
        </div>
        <video id="teaser" autoplay="" muted="" loop="" playsinline="" controls="" width="90%"
          style="display: block; margin: auto;">
          <source src="./web/video/demo1.mov" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </section> -->

  <section class="hero teaser">
    <div class="hero-body">
      <div class="container is-max-desktop">

        <!-- Dataset Viewer -->
        <h2 class="title is-2" style="text-align: center;">Dataset Viewer</h2>
        <iframe src="https://huggingface.co/datasets/SayNext/SayNext-PC2K/embed/viewer/default/train" frameborder="0"
          width="100%" height="560px"></iframe>
        <br />
        <br />
        <!-- Dataset Viewer -->

        <!-- Leaderboard -->
        <br />
        <h2 class="title is-2" style="text-align: center;">Leaderboard</h2>
        <ul class="nav nav-tabs" id="myTab" role="tablist">
          <li class="nav-item" role="presentation">
            <button class="nav-link active" id="main-results-tab" data-bs-toggle="tab"
              data-bs-target="#saynext-pc2k-table-content" type="button" role="tab" aria-controls="main-results-tab"
              aria-selected="true">SayNext-PC2K</button>
          </li>
          <li class="nav-item" role="presentation">
            <button class="nav-link" id="saynext-pc19k-table-tab" data-bs-toggle="tab"
              data-bs-target="#saynext-pc19k-table-content" type="button" role="tab"
              aria-controls="saynext-pc19k-table-tab" aria-selected="false">SayNext-PC19K</button>
          </li>
          <li class="nav-item" role="presentation">
            <button class="nav-link" id="subject-independent-table-tab" data-bs-toggle="tab"
              data-bs-target="#subject-independent-table-content" type="button" role="tab"
              aria-controls="subject-independent-table-tab" aria-selected="false">Subject Independent</button>
          </li>
          <li class="nav-item" role="presentation">
            <button class="nav-link" id="cross-scenarios-table-tab" data-bs-toggle="tab"
              data-bs-target="#cross-scenarios-table-content" type="button" role="tab"
              aria-controls="cross-scenarios-table-tab" aria-selected="false">Cross Scenarios</button>
          </li>
        </ul>
        <div class="tab-content" id="myTabContent">
          <div class="tab-pane fade show active" id="saynext-pc2k-table-content" role="tabpanel"
            aria-labelledby="saynext-pc2k-table-content">
            <div id="saynext-pc2k-main-table"></div>
          </div>
          <div class="tab-pane fade" id="saynext-pc19k-table-content" role="tabpanel"
            aria-labelledby="saynext-pc19k-table-content">
            <div id="saynext-pc19k-main-table"></div>
          </div>
          <div class="tab-pane fade" id="subject-independent-table-content" role="tabpanel"
            aria-labelledby="subject-independent-table-content">
            <div id="subject-independent-main-table"></div>
          </div>
          <div class="tab-pane fade" id="cross-scenarios-table-content" role="tabpanel"
            aria-labelledby="cross-scenarios-table-content">
            <div id="cross-scenarios-main-table"></div>
          </div>
        </div>
        <br />
        <!-- Leaderboard -->

        <!-- Performance -->
        <br />
        <h2 class="title is-2" style="text-align: center">SayNext-Chat Performance</h2>
        <br />
        <div class="content has-text-justified"
          style="display: flex; gap: 0px; align-items: flex-start; margin-bottom: 0;">
          <figure style="flex: 0 0 45%; text-align: center; margin: 0;">
            <img src="web/img/benchmark.png" style="width: 100%; padding: 10px 0;" alt="SayNext-Chat Performance" />
            <!-- <figcaption style="font-size: 0.9em; text-align: center;">
              Title
            </figcaption> -->
          </figure>

          <figure style="flex: 1; text-align: center; margin: 0;">
            <img src="web/img/radar.png" style="width: 100%; padding: 10px 0;" alt="SayNext-Chat Performance" />
            <!-- <figcaption style="font-size: 0.9em; text-align: center;">
              Title
            </figcaption> -->
          </figure>
        </div>

        <div class="content has-text-justified">
          <figure style="text-align: center; margin: 0;">
            <img src="web/img/bars.png" style="width: 95%; padding: 10px 0;" alt="SayNext-Chat Performance" />
            <!-- <figcaption style="font-size: 0.9em; text-align: center;">
              Title
            </figcaption> -->
          </figure>
        </div>
        <!-- Performance -->


        <!-- Experimental Findings -->
        <br />
        <h2 class="title is-2" style="text-align: center;">Experimental Findings</h2>
        <div class="content has-text-justified">
          <ul class="content-larger-text">
            <li><b>Clear improvements with vision modality:</b> Incorporating visual cues consistently improves
              next-utterance
              prediction performance.</li>

            <li><b>SayNext-Chat outperforms baseline MLLMs:</b> Across all three evaluation dimensions, SayNext-Chat
              consistently
              surpasses zero-shot baselines, including frontier large-scale MLLMs, open-source models of comparable
              scale,
              and emotion-specific MLLMs.</li>

            <li><b>Priming vectors significantly boost emotional alignment:</b> While fine-tuning on domain-specific
              corpora
              increases both lexical overlap and semantic similarity, priming tokens further improve emotion accuracy of
              future utterances by 3%.</li>

            <li><b>Cross-scenario generalization and scalability:</b> SayNext-Chat maintains superior performance over
              compared
              baselines when evaluated on larger-scale datasets and across different scenarios in the zero-shot setting.
            </li>

            <li><b>Efficacy in human & LLM evaluations:</b> SayNext-Chat achieves higher scores in subjective human
              assessments,
              slightly surpassing GPT-4o and showing a clear margin over open-source MLLMs with comparable parameter
              scales.</li>
          </ul>
          <!-- Experimental Findings -->


          <!-- Framework -->
          <h2 class="title is-2" style="text-align: center">SayNext-Chat Framework</h2>
          <br />
          <div class="content has-text-justified" style="display: flex; align-items: center; gap: 5%; padding: 0 15%;">
            <figure style="flex: 0 0 45%; margin: 0;">
              <img src="web/img/framework.png" style="width: 100%; padding: 10px 0;"
                alt="SayNext-Bench performance overview." class="EAgent_overview_image" />
            </figure>

            <span style="flex: 1; margin: 0;" class="content-larger-text">
              <p>
                Inspired by a cognitive neuroscience perspective, we propose a dual-route prediction framework,
                SayNext-Chat, to anticipate forthcoming utterances, which incorporates learnable priming tokens
                representing the high-level belief priors from visual inputs and low-level cues directly perceived
                from multimodal inputs.
              </p>
              <br />
              <p>
                We design SayNext-Chat with two complementary predictive routes: a <b style="color: #5674cc;">fast
                  route</b>
                that directly maps
                low-level visual and textual cues to a response, and a <b style="color: #b3150e;">deep route</b> that
                infers
                high-level priors
                (priming factors) to guide generation.
              </p>
            </span>
          </div>
          <!-- Framework -->

          <!-- Case Study -->
          <h2 class="title is-2" style="text-align: center">Case Study</h2>
          <br />
          <div class="content has-text-justified">
            <figure>
              <img src="web/img/sample.png" style="padding: 10px 0px" alt="SayNext-Bench performance overview." />
              <!-- <figcaption>
              <b>Figure 2:</b> Case Study
            </figcaption> -->
            </figure>
          </div>
          <!-- Case Study -->


        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre>
        @inproceedings{}
        </pre>
    </div>
  </section> -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="content is-small">
          This website templated is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a> and
          <a href="https://github.com/embodied-agent-interface/embodied-agent-interface.github.io/">eai</a>.
        </div>
      </div>
    </div>
  </footer>
</body>

</html>