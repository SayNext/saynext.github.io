<!DOCTYPE html>
<html>

<head>
  <title>
    SAYNEXT: A Benchmark and Cognitively Inspired Framework for Next-Utterance
    Prediction with Multimodal LLMs
  </title>
  <!-- consider to add our icon here -->
  <!-- <link rel="icon" href="" type="image/icon type"> -->

  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <script src="https://cdn.jsdelivr.net/npm/chart.js@4.4.0/dist/chart.umd.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2.0.0"></script>
  <script
    src="https://cdn.jsdelivr.net/npm/chartjs-plugin-annotation@3.0.1/dist/chartjs-plugin-annotation.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet" />
  <link rel="stylesheet" href="web/css/bulma.min.css" />
  <link rel="stylesheet" href="web/css/bulma-carousel.min.css" />
  <link rel="stylesheet" href="web/css/bulma-slider.min.css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" />
  <link rel="stylesheet" href="web/css/fontawesome.all.min.css" />

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="./web/js/bulma-carousel.min.js"></script>
  <script src="./web/js/bulma-slider.min.js"></script>

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous" />
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
    crossorigin="anonymous"></script>

  <link href="https://unpkg.com/tabulator-tables@5.5.2/dist/css/tabulator_bootstrap4.min.css" rel="stylesheet" />
  <script type="text/javascript" src="https://unpkg.com/tabulator-tables@5.5.2/dist/js/tabulator.min.js"></script>
  <script defer src="web/js/fontawesome.all.min.js"></script>

  <!-- below we load some js scripts -->
  <script src="web/js/leaderboard.js" type="module"></script>

  <link rel="stylesheet" href="web/css/index.css" />

  <!-- MathJax script -->
  <!-- <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script type="text/javascript">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [
          ["$", "$"],
          ["\\(", "\\)"],
        ],
      },
    });
  </script> -->

  <script>
    document.addEventListener("DOMContentLoaded", function () {
      var toggles = document.querySelectorAll(".toggle-section");
      toggles.forEach(function (toggle) {
        toggle.addEventListener("click", function () {
          var content = document.getElementById(
            toggle.getAttribute("aria-controls")
          );
          var toggleIcon = toggle.children[1].children[0];
          content.classList.toggle("is-active");
          if (content.classList.contains("is-active")) {
            toggleIcon.style.transition = "transform 0.3s ease";
            toggleIcon.style.transform = "rotate(180deg)";
          } else {
            toggleIcon.style.transition = "transform 0.3s ease";
            toggleIcon.style.transform = "rotate(0deg)";
          }
        });
      });
    });
  </script>

  <style>
    .collapse-content {
      display: none;
      margin-top: 10px;
    }

    .collapse-content.is-active {
      display: block;
    }

    /* .toggle-section .icon.is-small {
          transition: transform 0.3s ease;
        } */
    /* .toggle-section .fa-angle-up {
          transform: rotate(180deg);
        } */
  </style>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title is-bold">
              <!-- <img src="website/img/mint-leaf-logo.png" alt="logo" width="40" height="40" /> -->
              SayNext-Bench: Why Do LLMs Struggle with Next-Utterance Prediction?
            </h1>
            <!-- <br/> -->
            <p style="color: darkred; font-weight: bold; font-size: larger;">The First Benchmark for Predicting Human Next Utterance with MLLMs</p>

            <!-- <div class="is-size-5 publication-authors">
                        <span class="author-block">
                            <a href="???">???</a><sup>1</sup>,</span>
                        </div>

                        <div class="'is-size-4 publication-authors">
                        <span class="author-block" style="color: darkred; font-weight: bold;">Conference</span>
                        </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                                <a class="btn btn-outline-dark"
                                    role="button">
                                &nbsp;
                                    <i class="fas fa-file-pdf"></i>
                                    <span>&nbsp;&nbsp;Paper (Coming Soon)</span>
                                </a> &nbsp;&nbsp;
                                </span> -->

                <!-- Arxiv Link. -->
                <!-- <span class="link-block">
                                <a href="https://arxiv.org/abs/" class="btn btn-outline-dark"
                                    role="button">
                                    <span class="icon">
                                        <i class="ai ai-arxiv"></i>
                                    </span>
                                    <span>arXiv</span>
                                </a> &nbsp;&nbsp;
                            </span> -->

                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/SayNext/SayNext" class="btn btn-outline-dark" role="button">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  &nbsp;&nbsp;
                </span>

                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/SayNext/SayNext-PC2K" class="btn btn-outline-dark"
                    role="button" style="display: inline-flex; align-items: center">
                    <span class="icon" style="display: inline-flex; align-items: center">
                      <img src="https://huggingface.co/front/assets/huggingface_logo-noborder.svg" alt="Hugging Face"
                        style="width: 20px; height: 18px; margin-right: 5px" />
                    </span>
                    <span>Dataset</span>
                  </a>
                  &nbsp;&nbsp;
                </span>
              </div>
            </div>

            <!-- <br/> -->
            <br />

            <div class="content has-text-justified">
              <figure>
                <img src="web/img/main.png" style="padding: 10px 0px; width: 90%;"
                  alt="SayNext-Bench performance overview." />
                <figcaption style="text-align: left; display: block; width: 90%; margin: auto;">
                  <b>Figure 1:</b>
                  <b>The Illustration of Next-Utterance Prediction in SayNext-Bench.</b> Given a question utterance text
                  and the corresponding human reaction video, the task requires MLLMs to
                  predict the human’s subsequent response. Predicted responses from SayNext-Chat (<span
                    style="color: #009900">green</span>) are compared with ground-truth utterances (<span
                    style="color: #0066cc">blue</span>) and
                  other MLLMs (<span style="color: #cc0000">red</span>); key factors are extracted for interpretability.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="abstract">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-2">Abstract</h2>
          <div class="content has-text-justified content-larger-text">
            <p><b>Problem: </b>We explore the use of large language models (LLMs) for next-utterance prediction in human
              dialogue. Despite recent advances in LLMs demonstrating their ability to engage in natural conversations
              with users, we show that even leading models surprisingly struggle to predict a human speaker’s next
              utterance. Instead, humans can readily anticipate forthcoming utterances based on multi-modal cues—such as
              gestures, gaze, and emotional tone—from the context.
            </p>
            <p><b>Method: </b>
              To systematically examine whether LLMs can reproduce this ability, we propose <b>SayNext-Bench</b>, a
              benchmark that evaluates LLMs and Multimodal LLMs (MLLMs) on anticipating context-conditioned responses
              from multimodal cues spanning a variety of real-world scenarios.
              To support this benchmark, we build <b>SayNext-PC</b>, a novel large-scale dataset containing dialogues
              with rich multimodal cues.
              Building on this, we further develop a dual-route prediction MLLM, <b>SayNext-Chat</b>, that
              incorporates cognitively inspired design to emulate the predictive processing in conversation.
              Experimental results demonstrate that our model outperforms state-of-the-art MLLMs in terms of lexical
              overlap, semantic similarity, and emotion consistency.
            </p>
            <p><b>Contribution: </b>
              Our results prove the feasibility of next-utterance
              prediction with LLMs from multimodal cues and emphasize the (i) indispensable role of multimodal cues and
              (ii) actively predictive processing as the foundation of natural human interaction but missing in the
              current MLLMs. We hope that this exploration offers a new research entry toward a more human-like,
              context-sensitive AI
              interaction for human-centered AI.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-max-desktop">
      <h2 class="title is-2" style="text-align: center">Interactive Demo</h2>
      <div class="hero-body">
        <div style="width: 90%; margin: auto;">
          <p><b>Tip: </b>For the best experience, please turn on the audio while watching the demo.</p>
        </div>
        <video id="teaser" autoplay="" muted="" loop="" playsinline="" controls="" width="90%"
          style="display: block; margin: auto;">
          <source src="./web/video/demo1.mov" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </div>
  </section> -->

  <section class="hero teaser">
    <div class="hero-body">
      <div class="container is-max-desktop">

        <!-- Dataset Viewer -->
        <h2 class="title is-2" style="text-align: center;">Dataset Viewer</h2>
        <iframe src="https://huggingface.co/datasets/SayNext/SayNext-PC2K/embed/viewer/default/train" frameborder="0"
          width="100%" height="560px"></iframe>
        <br />
        <br />
        <!-- Dataset Viewer -->

        <!-- Leaderboard -->
        <br />
        <h2 class="title is-2" style="text-align: center;">Leaderboard</h2>
        <ul class="nav nav-tabs" id="myTab" role="tablist">
          <li class="nav-item" role="presentation">
            <button class="nav-link active" id="main-results-tab" data-bs-toggle="tab"
              data-bs-target="#saynext-pc2k-table-content" type="button" role="tab" aria-controls="main-results-tab"
              aria-selected="true">SayNext-PC2K</button>
          </li>
          <li class="nav-item" role="presentation">
            <button class="nav-link" id="saynext-pc19k-table-tab" data-bs-toggle="tab"
              data-bs-target="#saynext-pc19k-table-content" type="button" role="tab"
              aria-controls="saynext-pc19k-table-tab" aria-selected="false">SayNext-PC19K</button>
          </li>
          <li class="nav-item" role="presentation">
            <button class="nav-link" id="subject-independent-table-tab" data-bs-toggle="tab"
              data-bs-target="#subject-independent-table-content" type="button" role="tab"
              aria-controls="subject-independent-table-tab" aria-selected="false">Subject Independent</button>
          </li>
          <li class="nav-item" role="presentation">
            <button class="nav-link" id="cross-scenarios-table-tab" data-bs-toggle="tab"
              data-bs-target="#cross-scenarios-table-content" type="button" role="tab"
              aria-controls="cross-scenarios-table-tab" aria-selected="false">Cross Scenarios</button>
          </li>
        </ul>
        <div class="tab-content" id="myTabContent">
          <div class="tab-pane fade show active" id="saynext-pc2k-table-content" role="tabpanel"
            aria-labelledby="saynext-pc2k-table-content">
            <div id="saynext-pc2k-main-table"></div>
          </div>
          <div class="tab-pane fade" id="saynext-pc19k-table-content" role="tabpanel"
            aria-labelledby="saynext-pc19k-table-content">
            <div id="saynext-pc19k-main-table"></div>
          </div>
          <div class="tab-pane fade" id="subject-independent-table-content" role="tabpanel"
            aria-labelledby="subject-independent-table-content">
            <div id="subject-independent-main-table"></div>
          </div>
          <div class="tab-pane fade" id="cross-scenarios-table-content" role="tabpanel"
            aria-labelledby="cross-scenarios-table-content">
            <div id="cross-scenarios-main-table"></div>
          </div>
        </div>
        <br />
        <!-- Leaderboard -->

        <!-- Performance -->
        <br />
        <h2 class="title is-2" style="text-align: center">SayNext-Chat Performance</h2>
        <br />
        <div class="content has-text-justified"
          style="display: flex; gap: 0px; align-items: flex-start; margin-bottom: 0;">
          <figure style="flex: 0 0 45%; text-align: center; margin: 0;">
            <img src="web/img/benchmark.png" style="width: 100%; padding: 10px 0;" alt="SayNext-Chat Performance" />
            <!-- <figcaption style="font-size: 0.9em; text-align: center;">
              Title
            </figcaption> -->
          </figure>

          <figure style="flex: 1; text-align: center; margin: 0;">
            <img src="web/img/radar.png" style="width: 100%; padding: 10px 0;" alt="SayNext-Chat Performance" />
            <!-- <figcaption style="font-size: 0.9em; text-align: center;">
              Title
            </figcaption> -->
          </figure>
        </div>

        <div class="content has-text-justified">
          <figure style="text-align: center; margin: 0;">
            <img src="web/img/bars.png" style="width: 95%; padding: 10px 0;" alt="SayNext-Chat Performance" />
            <!-- <figcaption style="font-size: 0.9em; text-align: center;">
              Title
            </figcaption> -->
          </figure>
        </div>
        <!-- Performance -->


        <!-- Experimental Findings -->
        <br />
        <h2 class="title is-2" style="text-align: center;">Experimental Findings</h2>
        <div class="content has-text-justified">
          <ul class="content-larger-text">
            <li><b>Clear improvements with vision modality:</b> Incorporating visual cues consistently improves
              next-utterance
              prediction performance.</li>

            <li><b>SayNext-Chat outperforms baseline MLLMs:</b> Across all three evaluation dimensions, SayNext-Chat
              consistently
              surpasses zero-shot baselines, including frontier large-scale MLLMs, open-source models of comparable
              scale,
              and emotion-specific MLLMs.</li>

            <li><b>Priming vectors significantly boost emotional alignment:</b> While fine-tuning on domain-specific
              corpora
              increases both lexical overlap and semantic similarity, priming tokens further improve emotion accuracy of
              future utterances by 3%.</li>

            <li><b>Cross-scenario generalization and scalability:</b> SayNext-Chat maintains superior performance over
              compared
              baselines when evaluated on larger-scale datasets and across different scenarios in the zero-shot setting.
            </li>

            <li><b>Efficacy in human & LLM evaluations:</b> SayNext-Chat achieves higher scores in subjective human
              assessments,
              slightly surpassing GPT-4o and showing a clear margin over open-source MLLMs with comparable parameter
              scales.</li>
          </ul>
          <!-- Experimental Findings -->


          <!-- Framework -->
          <h2 class="title is-2" style="text-align: center">SayNext-Chat Framework</h2>
          <br />
          <div class="content has-text-justified" style="display: flex; align-items: center; gap: 5%; padding: 0 15%;">
            <figure style="flex: 0 0 45%; margin: 0;">
              <img src="web/img/framework.png" style="width: 100%; padding: 10px 0;"
                alt="SayNext-Bench performance overview." class="EAgent_overview_image" />
            </figure>

            <span style="flex: 1; margin: 0;" class="content-larger-text">
              <p>
                Inspired by a cognitive neuroscience perspective, we propose a dual-route prediction framework,
                SayNext-Chat, to anticipate forthcoming utterances, which incorporates learnable priming tokens
                representing the high-level belief priors from visual inputs and low-level cues directly perceived
                from multimodal inputs.
              </p>
              <br />
              <p>
                We design SayNext-Chat with two complementary predictive routes: a <b style="color: #5674cc;">fast
                  route</b>
                that directly maps
                low-level visual and textual cues to a response, and a <b style="color: #b3150e;">deep route</b> that
                infers
                high-level priors
                (priming factors) to guide generation.
              </p>
            </span>
          </div>
          <!-- Framework -->

          <!-- Case Study -->
          <h2 class="title is-2" style="text-align: center">Case Study</h2>
          <br />
          <div class="content has-text-justified">
            <figure>
              <img src="web/img/sample.png" style="padding: 10px 0px" alt="SayNext-Bench performance overview." />
              <!-- <figcaption>
              <b>Figure 2:</b> Case Study
            </figcaption> -->
            </figure>
          </div>
          <!-- Case Study -->


        </div>
      </div>
    </div>
  </section>

  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre>
        @inproceedings{}
        </pre>
    </div>
  </section> -->

  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="content is-small">
          This website templated is borrowed from <a href="https://github.com/nerfies/nerfies.github.io">nerfies</a> and
          <a href="https://github.com/embodied-agent-interface/embodied-agent-interface.github.io/">eai</a>.
        </div>
      </div>
    </div>
  </footer>
</body>

</html>